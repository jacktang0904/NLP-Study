{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMOqmt6jbgbx",
        "outputId": "a0a88523-7094-408e-90a6-315b6b2a2b81"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/Colab Notebooks\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.FloatTensor\n",
        "print('will use', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "will use cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPGHTNxib0Jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e39a24-4442-4d7a-a6be-47170b0600e4"
      },
      "source": [
        "with open('text8.train.txt') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = text.lower().split()\n",
        "vocab = dict(Counter(text).most_common(9999))\n",
        "vocab['UNK'] = len(text) - np.sum(list(vocab.values()))\n",
        "vocab_size = len(vocab)\n",
        "# build vocabulary\n",
        "token2idx = {t: i for i, t in enumerate(vocab)}\n",
        "idx2token = {i: t for i, t in enumerate(vocab)}\n",
        "\n",
        "word_count = np.array([cnt for cnt in vocab.values()])\n",
        "word_freqs = word_count / np.sum(word_count)\n",
        "word_freqs = word_freqs ** 0.75\n",
        "print(word_freqs[: 3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.12530107 0.08108672 0.06208324]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oaj-537eK2Y"
      },
      "source": [
        "C = 3\n",
        "K = 5\n",
        "\n",
        "class Word2VecDataset(Data.Dataset):\n",
        "    def __init__(self, text, token2idx, word_freqs):\n",
        "        super().__init__()\n",
        "        self.text_encoded = [token2idx.get(token, token2idx['UNK']) for token in text]\n",
        "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
        "        self.word_freqs = torch.Tensor(word_freqs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_encoded)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        center_words = self.text_encoded[idx]\n",
        "        context_idx = list(range(idx - C, idx)) + list(range(idx + 1, idx + 1 + C))\n",
        "        context_idx = [i % len(self.text_encoded) for i in context_idx]\n",
        "        context_words = self.text_encoded[context_idx]\n",
        "        # print(context_words.shape)\n",
        "        neg_words = torch.multinomial(self.word_freqs, K * len(context_words), True)\n",
        "        while len(set(context_words) & set(neg_words)) > 0:\n",
        "            neg_words = torch.multinomial(self.word_freqs, K * len(context_words), True)\n",
        "\n",
        "        return center_words, context_words, neg_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-EHRo1DjMQ4"
      },
      "source": [
        "batch_size = 256\n",
        "dataset = Word2VecDataset(text, token2idx, word_freqs)\n",
        "data_loader = Data.DataLoader(dataset, batch_size, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxckTXG5N6To",
        "outputId": "229c3b42-483f-4930-a971-817e2be6a538"
      },
      "source": [
        "next(iter(dataset))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(4813),\n",
              " tensor([  50, 9999,  393, 3139,   11,    5]),\n",
              " tensor([ 787,   19,  140, 7325, 5804, 3759,   38, 1884, 2395, 1284, 3834, 1648,\n",
              "           16,  947, 9999,    1,  829, 9910, 3639,    0,    2, 6334,   50,  586,\n",
              "         1091, 1156,  153, 1414, 2747, 3793]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvmfmIILjnKE"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.in_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    def forward(self, center_words, context_words, neg_words):\n",
        "        center_embed = self.in_embedding(center_words)\n",
        "        context_embed = self.out_embedding(context_words)\n",
        "        neg_embed = self.out_embedding(neg_words)\n",
        "\n",
        "        center_embed = center_embed.unsqueeze(2)\n",
        "        \n",
        "        pos_dot = torch.bmm(context_embed, center_embed)\n",
        "        neg_dot = torch.bmm(-neg_embed, center_embed)\n",
        "\n",
        "        pos_dot = pos_dot.squeeze(2)\n",
        "        neg_dot = neg_dot.squeeze(2)\n",
        "\n",
        "        loss_pos = F.logsigmoid(pos_dot).sum(1)\n",
        "        loss_neg = F.logsigmoid(neg_dot).sum(1)\n",
        "        return -(loss_pos + loss_neg)\n",
        "\n",
        "    def center_embedding(self):\n",
        "        return self.in_embedding.weight.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trzXu8Gvom4_"
      },
      "source": [
        "embedding_dim = 100\n",
        "model = Model(vocab_size, embedding_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1aTR5N1pDBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134e223b-a036-47de-9f28-ca2746baee6d"
      },
      "source": [
        "for i, (center_words, context_words, neg_words) in enumerate(data_loader):\n",
        "    center_words, context_words, neg_words = center_words.long().to(device), context_words.long().to(device), neg_words.long().to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(center_words, context_words, neg_words).mean().to(device)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 10000 == 0:\n",
        "        print('iteration', i, loss.item())\n",
        "\n",
        "# embedding_weights = model.center_embedding()\n",
        "torch.save(model.state_dict(), \"embedding-{}.th\".format(embedding_dim))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 149.3065948486328\n",
            "iteration 10000 14.635737419128418\n",
            "iteration 20000 13.880234718322754\n",
            "iteration 30000 14.170469284057617\n",
            "iteration 40000 13.659147262573242\n",
            "iteration 50000 13.598966598510742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5oUM_vWHRmr"
      },
      "source": [
        "embedding_weights = model.center_embedding()\n",
        "def find_nearest(word):\n",
        "    index = token2idx[word]\n",
        "    embedding = embedding_weights[index]\n",
        "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
        "    return [idx2token[i] for i in cos_dis.argsort()[: 3]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CUyGaQifMPi",
        "outputId": "4a3b4982-3e62-45ee-e571-630b2f5f58fa"
      },
      "source": [
        "for word in [\"two\", \"america\", \"computer\"]:\n",
        "    print(word, find_nearest(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "two ['two', 'four', 'three']\n",
            "america ['america', 'africa', 'europe']\n",
            "computer ['computer', 'computing', 'interface']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWdrlWApz4N4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}